Odds Ratio Preference Optimization (ORPO) is an innovative approach that integrates preference alignment directly into the supervised fine-tuning phase of a language model. It modifies the conventional negative log-likelihood loss by adding an extra penalty term based on the log odds ratio between a preferred (chosen) response and a non-preferred (rejected) one. This odds ratio is calculated by comparing the probability assigned to the chosen response against that of the rejected response, ensuring that the model learns to favor outputs that align with human preferences. By embedding this penalty into the loss function, ORPO effectively guides the model to increase the likelihood of generating desired responses while simultaneously discouraging undesired styles. Unlike traditional methods that require a separate reward model or multiple training stages, ORPO achieves alignment in a single, monolithic process, thereby reducing computational overhead. Empirical studies have shown that this method can lead to substantial improvements in performance across various benchmarks, even when applied to relatively small models. Its design not only preserves the benefits of supervised fine-tuning but also provides a stable gradient signal, avoiding the pitfalls of overly aggressive suppression that can occur with simpler probability ratio methods. Overall, ORPO represents a streamlined and efficient strategy for developing language models that are better aligned with human values and preferences.
