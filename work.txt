Group Relative Policy Optimization (GRPO) is a reinforcement learning algorithm developed by DeepSeek to optimize large language models (LLMs) efficiently. It replaces traditional value networks with group-based relative comparisons: for each input query, multiple responses are sampled, their rewards are normalized within the group (e.g., using Z-score standardization), and advantages are derived from these relative scores 3512. This approach eliminates the need for a separate critic model, significantly reducing memory and computational costs compared to methods like PPO
