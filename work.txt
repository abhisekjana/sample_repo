TRPO (Trust Region Policy Optimization) maximizes a surrogate objective that estimates the performance of the new policy using trajectories collected from the old policy. This surrogate objective reweights the advantages of past actions (via importance sampling) to approximate how beneficial they would be under the updated policy. However, to ensure updates remain stable and avoid catastrophic policy changes, TRPO constrains the new policy to stay within a trusted region of the old policy. The KL divergence measures the "distance" between the old and new policies, enforcing a hard limit (e.g., a threshold Î´) on how much the policy can change in a single update. This balance of aggressive optimization and constrained updates guarantees monotonic policy improvement.
