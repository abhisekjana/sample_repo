The reward model in RLHF (Reinforcement Learning from Human Feedback) is typically a transformer-based neural network trained to predict human preferences. Its architecture includes:

Base Model: Initialized from a pre-trained language model (e.g., a smaller version of the policy model) to leverage general language understanding.

Regression Head: A final linear layer added on top of the base model, which outputs a scalar reward score for a given (prompt, response) pair.

Input Format: The prompt and response are concatenated (e.g., [prompt] [response] [EOS]) and tokenized into a sequence.

Training Data: Uses human-annotated preference datasets (e.g., ranked responses to prompts) and is trained with a pairwise ranking loss (e.g., Bradley-Terry model) to learn relative preferences.

Output: A single numerical value representing alignment with human preferences (higher = better).

The model is frozen during RL training to provide stable reward signals for optimizing the policy (LLM).
