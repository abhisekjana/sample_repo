import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from langchain.schema import AIMessage

# Create a custom HuggingFacePipeline subclass that passes through the extra output.
class CustomHuggingFacePipeline(HuggingFacePipeline):
    def _generate(self, prompt: str, **kwargs):
        # Ensure that output_scores and return_dict_in_generate are enabled.
        result = self.pipeline(
            prompt,
            output_scores=True,
            return_dict_in_generate=True,
            **kwargs
        )
        # Return the full dictionary, which includes both generated text and scores.
        return result

# Create a custom ChatHuggingFace subclass that attaches the full output to the response metadata.
class CustomChatHuggingFace(ChatHuggingFace):
    def invoke(self, messages, **kwargs):
        # Convert messages to a prompt string (this depends on your prompt template)
        # For simplicity, we assume messages is a simple string prompt.
        prompt = messages if isinstance(messages, str) else str(messages)
        # Generate output using our custom pipeline.
        result = self.llm._generate(prompt, **kwargs)
        
        # Create an AIMessage with the generated text.
        # We assume the generated text is in result["sequences"].
        generated_text = result["sequences"][0]
        # Optionally, you can process the scores here.
        # For example, convert the logits of the first token to log probabilities:
        if "scores" in result and result["scores"]:
            first_token_logits = result["scores"][0]
            log_probs = F.log_softmax(torch.tensor(first_token_logits), dim=-1)
        else:
            log_probs = None

        # Build your message and attach the extra metadata.
        message = AIMessage(content=generated_text)
        message.response_metadata = {
            "full_output": result,
            "first_token_log_probs": log_probs,
        }
        return message

# Example usage:
model_name = "gpt2"  # Replace with your local model's identifier or path
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, local_files_only=True)

# Create a Hugging Face pipeline with the desired settings.
hf_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=50,
    # Do not forget to set these so that scores are returned!
    output_scores=True,
    return_dict_in_generate=True,
)

# Wrap it using our custom wrapper.
custom_pipeline = CustomHuggingFacePipeline(pipeline=hf_pipeline)
custom_chat_model = CustomChatHuggingFace(llm=custom_pipeline)

# Invoke the model.
response = custom_chat_model.invoke("How are you today?")
print("Response:", response.content)
if response.response_metadata["first_token_log_probs"] is not None:
    print("Log probabilities for first token:", response.response_metadata["first_token_log_probs"])
