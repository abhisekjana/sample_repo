python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Meta-Llama-3.1-70B-Instruct  \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size <N>  # N = number of GPUs if using multiple
    # --quantization awq # Optional: if using AWQ quantization
    # --gpu-memory-utilization 0.9 # Optional: control VRAM usage
    # Add other args as needed (dtype, max_model_len, etc.)


API Endpoint: http://localhost:8000/v1

# --- Inside your LangGraph Python script ---
from langchain_openai import ChatOpenAI

# --- Configuration ---
# Example using vLLM default endpoint:
LOCAL_API_BASE_URL = "http://localhost:8000/v1"
# Model name as expected by the vLLM server (usually the HF identifier)
LOCAL_MODEL_NAME = "meta-llama/Meta-Llama-3.1-70B-Instruct"
# API Key usually not needed for local servers
LOCAL_API_KEY = "NA"

# --- Inside your call_openai_api node function ---
chat_model = ChatOpenAI(
    model=LOCAL_MODEL_NAME,
    openai_api_base=LOCAL_API_BASE_URL,
    openai_api_key=LOCAL_API_KEY,
    temperature=0.6,
    max_tokens=512,
    model_kwargs={
        "logprobs": True,
        # Add other compatible parameters if needed
    }
)

============

import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_huggingface import ChatHuggingFace # Use the specific Chat wrapper
from langchain_core.messages import HumanMessage, SystemMessage

# --- Load Model and Tokenizer (as before) ---
model_name = "gpt2" # Replace with your actual model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
device = model.device # Or specify 'cuda', 'cpu'

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

# --- Instantiate ChatHuggingFace ---
# Pass generate kwargs via model_kwargs to request scores
llm = ChatHuggingFace(
    model=model,
    tokenizer=tokenizer,
    model_kwargs={
        "max_new_tokens": 50,
        "temperature": 0.7,
        "do_sample": True,
        # --- Crucial for Log Probs ---
        "output_scores": True,
        "return_dict_in_generate": True,
    },
    task="text-generation" # Explicitly setting task can sometimes help
)

# --- Prepare Messages ---
messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="Explain the concept of log probabilities in LLMs.")
]

# --- Invoke the Model ---
response = llm.invoke(messages)

# --- Extract Response and Log Probabilities ---
response_text = response.content
log_probs_list = []

print(f"Response Text: {response_text}")

# Log probs are usually in response_metadata
metadata = response.response_metadata
# print("\nFull Metadata:", metadata) # Uncomment to inspect structure

if metadata and 'scores' in metadata and 'sequences' in metadata:
    scores = metadata['scores'] # Tuple of logit tensors (batch, vocab_size) per step
    # Get generated sequence, excluding input tokens
    # NOTE: Need input length. ChatHuggingFace might not easily expose this.
    # We might need to re-tokenize the input messages to find the length,
    # or make assumptions based on the structure.
    # Let's assume we can get the generated IDs somehow (this is the tricky part with ChatHuggingFace)
    # Often, the metadata *doesn't* directly contain the raw sequence needed to align scores.
    # This highlights a limitation of relying solely on ChatHuggingFace metadata for this.

    # --- Alternative within metadata (Less common but check) ---
    # Sometimes metadata might contain pre-processed logprobs directly
    if 'logprobs' in metadata: # Check for a pre-processed key
         # Process based on the structure provided in metadata['logprobs']
         # This depends heavily on how ChatHuggingFace populates it.
         print("Found 'logprobs' key in metadata - structure needs inspection.")
         # Example hypothetical structure:
         # if isinstance(metadata['logprobs'], list):
         #    for item in metadata['logprobs']: # e.g., [{'token': '...', 'logprob': ...}]
         #       log_probs_list.append((item.get('token'), item.get('logprob')))

    else:
        print("\nLog probabilities (scores) found, but aligning them with tokens ")
        print("using only ChatHuggingFace metadata can be complex.")
        print("Consider Solution 2 for precise token-level logprob alignment.")

else:
    print("\nScores or sequences not found in response_metadata. Ensure model_kwargs are set correctly.")

# --- Print extracted log probs (if available/processed) ---
if log_probs_list:
    print("\nProcessed Log Probabilities:")
    for token, lp in log_probs_list[:10]: # Print first few
        print(f"  '{token}': {lp:.4f}")

==============


import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage # Can still use for structure

# --- Load Model and Tokenizer (as before) ---
model_name = "gpt2" # Replace with your actual model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
device = model.device

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

# --- Prepare Messages (using LangChain structure is fine) ---
messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="Explain the concept of log probabilities in LLMs.")
]

# --- Manually Apply Chat Template ---
# Convert LangChain messages to the list of dicts format expected by apply_chat_template
formatted_messages = [{"role": msg.type if msg.type != 'ai' else 'assistant', "content": msg.content} for msg in messages]

# Tokenize using the chat template
# add_generation_prompt=True is crucial for instruct/chat models
input_ids = tokenizer.apply_chat_template(
    formatted_messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(device)
input_length = input_ids.shape[1]

# --- Call model.generate() directly ---
generation_kwargs = dict(
    input_ids=input_ids,
    max_new_tokens=50,
    temperature=0.7,
    do_sample=True,
    # --- Crucial for Log Probs ---
    output_scores=True,
    return_dict_in_generate=True,
)

print("Calling model.generate()...")
with torch.no_grad():
    outputs = model.generate(**generation_kwargs)

# --- Process Output and Log Probabilities ---
# Decode only the newly generated tokens
generated_ids = outputs.sequences[0, input_length:]
response_text = tokenizer.decode(generated_ids, skip_special_tokens=True)

print(f"\nResponse Text: {response_text}")

# Extract and calculate log probabilities for chosen tokens
log_probs_list = []
if outputs.scores:
    # scores is a tuple of tensors, one per generated token step
    # Each tensor has shape (batch_size, vocab_size) with raw logits
    for i, score_tensor in enumerate(outputs.scores):
        # Apply log_softmax to get log probabilities
        log_probs = F.log_softmax(score_tensor, dim=-1) # Shape: (batch_size, vocab_size)

        # Get the ID of the token that was actually chosen at this step
        # Ensure index 'i' is valid for generated_ids
        if i < len(generated_ids):
            generated_token_id = generated_ids[i].item() # Get the token ID as an integer

            # Get the log probability of the chosen token
            # log_probs[0] because batch_size is 1
            token_log_prob = log_probs[0, generated_token_id].item() # Get as float

            # Also get the token string for readability
            token_string = tokenizer.decode([generated_token_id])
            log_probs_list.append((token_string, generated_token_id, token_log_prob))
        else:
            print(f"Warning: Score index {i} out of bounds for generated_ids length {len(generated_ids)}")


# --- Print Log Probs ---
if log_probs_list:
    print("\nToken Log Probabilities (Token, ID, LogProb):")
    for token_str, token_id, lp in log_probs_list[:10]: # Print first few
        print(f"  '{token_str}' ({token_id}): {lp:.4f}")
else:
    print("\nCould not extract log probabilities.")

=========
