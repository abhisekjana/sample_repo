1. The Challenge: Achieving High Confidence in LLM Extractions
Context: Our OCR -> LLM pipeline achieves high overall accuracy (95%) for entity extraction from scanned documents.
Problem: Identifying the remaining ~5% of incorrect extractions is crucial for downstream reliability and preventing propagation of errors. Relying solely on raw LLM outputs (like log probabilities) often falls short.
Why Raw Scores Are Insufficient:
LLM log probabilities primarily reflect token-level fluency or likelihood, not necessarily factual correctness or semantic validity within the document context ([Xiong et al., 2024, p.2] note limitations of token-likelihood capturing semantic falsity).
Simple thresholds on a single metric (e.g., minimum logprob) lack robustness – they can miss errors caused by factors other than low LLM sequence probability (e.g., subtle OCR errors the LLM confidently misinterprets, or confident hallucinations).
While LLMs may internally "know what they know," simple surface-level scores don't always capture this nuanced self-assessment reliably across diverse scenarios ([Kadavath et al., 2022]).
2. Our Approach: A Discriminative GBM Model for Confidence Scoring
Strategy: Instead of relying on a single heuristic, we train a Gradient Boosting Model (GBM) specifically to discriminate between correctly and incorrectly extracted entities.
Leveraging Diverse Signals: The GBM integrates information from multiple sources:
LLM Internal State: Log probabilities (min, avg, stddev, distribution shape) – Captures the model's generation confidence/fluency.
Input Quality: AWS Textract word confidences (min, avg, stddev, distribution) – Assesses the reliability of the input text the LLM processed.
Document Context: Document Type – Provides high-level context about expected entity types and formats.
(Future Potential): Easily extensible with image features, layout data, entity type constraints, knowledge base lookups, etc.
3. Why the GBM Approach is Superior:
Synergistic Feature Integration: The GBM learns complex interactions between features.
Example: It can identify risky extractions where the LLM is confident (high logprob) but the underlying OCR quality is poor (low Textract confidence), a scenario missed by logprob thresholds alone.
It can learn different confidence patterns based on document type.
Robustness to Varied Error Modes: Errors stem from multiple sources (OCR issues, LLM hallucinations, contextual misunderstandings). A multi-feature model is inherently more capable of capturing these diverse failure patterns than a single-feature heuristic.
Improved Empirical Performance: Our GBM model demonstrably improves error detection, identifying 60% of incorrect entities in our test set, proving more effective and robust than relying solely on the minimum log probability feature.
Foundation for Advanced Analysis: This approach aligns with broader uncertainty quantification efforts for LLMs, which recognize the need to analyze multiple facets of model behavior and input quality ([Huang et al., 2024] explore various uncertainty metrics).
4. Justification: Investing in Reliability
Overhead vs. Reliability: While the GBM introduces model training, hosting, and maintenance, this is a necessary investment for applications requiring high trustworthiness.
Limitations of "Model-Free": Simpler, "model-free" approaches based on single metrics (like min logprob) are less adaptable, struggle with complex error patterns, and ultimately provide lower confidence in the extracted entities, limiting their suitability for critical, wider applications ([Kadavath et al., 2022] implies richer information exists; [Xiong et al., 2024] & [Huang et al., 2024] highlight complexity and limitations of simple metrics). The robustness gained by the GBM justifies the operational overhead.
References:
[Kadavath et al., 2022] Kadavath, S., et al. "Language Models (Mostly) Know What They Know." arXiv:2207.05221. (Supports: LLMs have internal knowledge states, but simple scores may not capture them; need for better evaluation than raw scores).
[Xiong et al., 2024] Xiong, M., et al. "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs." ICLR 2024. (Supports: Limitations of token-likelihoods for semantic correctness; complexity of confidence elicitation).
[Huang et al., 2024] Huang, Y., et al. "Look Before You Leap: An Exploratory Study of Uncertainty Analysis for Large Language Models." IEEE Transactions on Software Engineering (preprint). (Supports: General approach of using uncertainty analysis for risk assessment; implies value in combining multiple metrics).
(Optional - General ML Principle): Could add a conceptual reference to a standard ML text highlighting the benefits of feature engineering and ensemble/boosting methods for capturing complex interactions and improving robustness over simple linear models or heuristics.
This slide structure logically builds the case: it starts with the problem (need for reliable confidence beyond basic scores), presents the GBM as a solution that integrates multiple signals, explains why this integration leads to superior performance and robustness (citing evidence and papers), and addresses the counter-argument (overhead) by framing it as a necessary cost for reliability.
