import pandas as pd
import numpy as np
import lightgbm as lgb
# Removed RandomizedSearchCV import
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from imblearn.over_sampling import RandomOverSampler
import math
import joblib
import warnings

warnings.filterwarnings('ignore', category=FutureWarning)

# --- Configuration ---
CSV_FILE_PATH = 'your_feature_data.csv'  # <<< CHANGE THIS TO YOUR CSV FILE PATH
TARGET_COLUMN = 'gt'
TEST_SIZE = 0.10
VALIDATION_SIZE = 0.10
USE_RESAMPLING = False
RESAMPLING_BINS = [0, 0.3, 0.7, 1.0]
RESAMPLING_STRATEGY = 'auto'
RANDOM_STATE = 42
MODEL_SAVE_PATH = 'lgbm_confidence_model_fixed.joblib' # <<< Path to save the final model

# --- FIXED Hyperparameters ---
# IMPORTANT: Replace these with good values found from previous tuning (like RandomizedSearch)
# or use as a starting point. These specific values might not be optimal.
FIXED_PARAMS = {
    'n_estimators': 500,       # Max trees, but early stopping will likely choose fewer
    'learning_rate': 0.05,
    'num_leaves': 40,
    'colsample_bytree': 0.8,
    'subsample': 0.8,
    'reg_alpha': 0.1,
    'reg_lambda': 0.1,
    'min_child_samples': 10,
    'min_split_gain': 0.0,
    'random_state': RANDOM_STATE,
    'n_jobs': -1                 # Use all available cores
}

# --- Helper Function for Resampling Binning ---
def create_bins(y, bins):
    binned_y = pd.cut(y, bins=bins, labels=False, include_lowest=True, right=True)
    binned_y = binned_y.fillna(len(bins) - 2)
    return binned_y.astype(int)

# --- Main Script ---
print("--- Starting Modeling Process (Fixed Parameters) ---")

# 1. Load Data
try:
    print(f"Loading data from: {CSV_FILE_PATH}")
    df = pd.read_csv(CSV_FILE_PATH)
    print(f"Data loaded successfully. Shape: {df.shape}")
except FileNotFoundError:
    print(f"Error: CSV file not found at {CSV_FILE_PATH}.")
    exit()
except Exception as e:
    print(f"Error loading CSV: {e}")
    exit()
if TARGET_COLUMN not in df.columns:
    print(f"Error: Target column '{TARGET_COLUMN}' not found.")
    exit()

# 2. Define Features (X) and Target (y)
print(f"Separating features and target ('{TARGET_COLUMN}')...")
X = df.drop(TARGET_COLUMN, axis=1)
y = df[TARGET_COLUMN]
print(f"Features shape: {X.shape}, Target shape: {y.shape}")
numeric_cols = X.select_dtypes(include=np.number).columns.tolist()
if len(numeric_cols) != X.shape[1]:
    print("Warning: Non-numeric columns detected...")
    # Optionally handle non-numeric columns (e.g., encode or drop)
    # X = X[numeric_cols]

# 3. Split Data (Train/Validation/Test)
print(f"Splitting data...")
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
)
val_size_relative = VALIDATION_SIZE / (1 - TEST_SIZE)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=val_size_relative, random_state=RANDOM_STATE
)
print(f"Train set size: {X_train.shape[0]}")
print(f"Validation set size: {X_val.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")
print(f"Training target variance: {y_train.var():.4f}")

# Check for zero-variance features
print("\nChecking for zero-variance features...")
variances = X_train.var()
zero_variance_cols = variances[variances == 0].index.tolist()
if zero_variance_cols:
    print(f"Warning: Found zero-variance columns: {zero_variance_cols}")
    print("Removing zero-variance columns...")
    cols_to_drop_train = [col for col in zero_variance_cols if col in X_train.columns]
    cols_to_drop_val = [col for col in zero_variance_cols if col in X_val.columns]
    cols_to_drop_test = [col for col in zero_variance_cols if col in X_test.columns]
    if cols_to_drop_train: X_train = X_train.drop(columns=cols_to_drop_train)
    if cols_to_drop_val: X_val = X_val.drop(columns=cols_to_drop_val)
    if cols_to_drop_test: X_test = X_test.drop(columns=cols_to_drop_test)
    print(f"Remaining features in train set: {X_train.shape[1]}")
else:
    print("No zero-variance features found.")

# 4. Resampling (Optional)
if USE_RESAMPLING:
    print("\n--- Applying Resampling ---")
    try:
        y_train_binned = create_bins(y_train, RESAMPLING_BINS)
        print("Original training distribution (based on bins):")
        print(y_train_binned.value_counts(normalize=True).sort_index())
        resampler = RandomOverSampler(sampling_strategy=RESAMPLING_STRATEGY, random_state=RANDOM_STATE)
        resampled_indices = resampler.fit_resample(X_train, y_train_binned)[1].index # Get indices correctly
        X_train = X_train.loc[resampled_indices]
        y_train = y_train.loc[resampled_indices]
        print(f"Resampled training data shape: {X_train.shape}")
        print("Resampled training distribution (based on bins):")
        print(create_bins(y_train, RESAMPLING_BINS).value_counts(normalize=True).sort_index())
        print("Training data has been replaced with resampled data.")
    except Exception as e:
        print(f"Error during resampling: {e}")
        print("Proceeding without resampling.")
        USE_RESAMPLING = False
else:
    print("\nSkipping resampling.")


# 5. Define and Train Model with Fixed Parameters <<< MODIFIED SECTION
print("\n--- Initializing LightGBM Model with Fixed Parameters ---")
print(f"Using parameters: {FIXED_PARAMS}")
model = lgb.LGBMRegressor(**FIXED_PARAMS)

print("\n--- Training Model ---")
# Train the model directly on the (potentially resampled) training data
# Use the validation set for early stopping to find the best number of trees
try:
    model.fit(X_train, y_train,
              eval_set=[(X_val, y_val)],
              eval_metric='rmse',
              callbacks=[lgb.early_stopping(10, verbose=True)] # Stop if val RMSE doesn't improve for 10 rounds
             )
    print("\nModel training finished.")
    print(f"Best iteration found by early stopping: {model.best_iteration_}")

except Exception as e:
     print(f"\nError during model fitting: {e}")
     print("Check data and parameters.")
     exit()

# 6. Evaluate on Test Set <<< Using 'model' directly
print("\n--- Evaluating Trained Model on Test Set ---")
y_pred_test = model.predict(X_test)

# Calculate metrics
test_rmse = math.sqrt(mean_squared_error(y_test, y_pred_test))
test_r2 = r2_score(y_test, y_pred_test)

print(f"Test Set RMSE: {test_rmse:.4f}")
print(f"Test Set R-squared (R2): {test_r2:.4f}")

# 7. Save the Trained Model <<< Using 'model' directly
print(f"\n--- Saving the trained model to {MODEL_SAVE_PATH} ---")
try:
    joblib.dump(model, MODEL_SAVE_PATH)
    print(f"Model saved successfully.")
except Exception as e:
    print(f"Error saving model: {e}")

# --- Optional: Feature Importance --- <<< Using 'model' directly
try:
    print("\n--- Feature Importances (Top 15) ---")
    feature_names = X_train.columns
    feature_importances = pd.Series(model.feature_importances_, index=feature_names)
    print(feature_importances.nlargest(15).to_string())
except Exception as e:
    print(f"Could not retrieve/display feature importances: {e}")


print("\n--- Modeling Process Complete ---")

# --- How to load the model later ---
# loaded_model = joblib.load(MODEL_SAVE_PATH)
# predictions = loaded_model.predict(new_X_data)
# print("Model loaded and ready for prediction.")

