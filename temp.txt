pip install autoawq transformers torch accelerate datasets sentencepiece



import os
import torch
from transformers import AutoTokenizer
from awq import AutoAWQForCausalLM # Import from awq library directly
from datasets import load_dataset # Optional: for calibration data

# --- Configuration ---
# 1. Path to your downloaded FP16/BF16 Llama 3.1 70B model
model_path = "/path/to/your/local/llama-3.1-70b-instruct"

# 2. Path where the quantized model will be saved
quant_path = "/path/to/save/quantized/llama-3.1-70b-instruct-awq"

# 3. Quantization configuration
quant_config = {
    "w_bit": 4,        # Target weight bit-width (4-bit is common)
    "q_group_size": 128, # Group size for quantization (128 is standard)
    "zero_point": True,  # Use zero-point quantization (common for AWQ)
    "version": "GEMM"    # Use GEMM version for general compatibility (alternative: "GEMV")
}

# 4. (Optional but Recommended) Calibration Data Configuration
use_calibration_data = True
calibration_dataset_name = "mit-han-lab/pile-val-backup" # A dataset for calibration
# Alternative: "wikitext", dataset="wikitext-2-raw-v1", split="train"
# Alternative: "allenai/c4", dataset="realnewslike", split="validation"
num_calibration_samples = 512 # Number of samples for calibration
calibration_text_column = "text" # Column containing text in the dataset

# --- Safety Check ---
if not os.path.isdir(model_path):
    raise FileNotFoundError(f"Original model path not found: {model_path}")
if os.path.exists(quant_path):
    print(f"Warning: Quantized model path already exists: {quant_path}")
    # Consider adding logic here to ask user or delete existing path

print(f"Loading original model from: {model_path}")
print("This may take a while and require significant RAM/VRAM...")

# --- Load Model and Tokenizer ---
# device_map="auto" distributes model across available GPUs/CPU RAM
# Requires 'accelerate' library
# Use bfloat16 if supported, otherwise float16 for loading
dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

try:
    # Initialize AutoAWQForCausalLM with the base model path and config
    # It handles loading the base model internally using transformers
    model = AutoAWQForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        safetensors=True, # Prefer SafeTensors format if available
        device_map="auto", # Crucial for large models
        torch_dtype=dtype,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    print("Original model and tokenizer loaded successfully.")
except Exception as e:
    print(f"Error loading original model/tokenizer: {e}")
    print("Ensure the model path is correct and you have enough system resources.")
    exit(1)


# --- Prepare Calibration Data (Optional) ---
calibration_data = None
if use_calibration_data:
    print(f"Loading calibration data from '{calibration_dataset_name}'...")
    try:
        dataset = load_dataset(calibration_dataset_name, split="validation", streaming=False) # Load a portion
        # Shuffle and select samples
        dataset = dataset.shuffle(seed=42).select(range(num_calibration_samples))
        # Extract text data - adapt column name if necessary
        calibration_data = [item[calibration_text_column] for item in dataset]
        print(f"Loaded {len(calibration_data)} samples for calibration.")
    except Exception as e:
        print(f"Warning: Failed to load calibration data: {e}")
        print("Proceeding without calibration data. Quality might be slightly reduced.")
        calibration_data = None # Fallback
else:
    print("Skipping calibration data.")


# --- Quantize the Model ---
print("Starting quantization process...")
print(f"Quantization config: {quant_config}")

try:
    model.quantize(
        tokenizer,
        quant_config=quant_config,
        calib_data=calibration_data # Pass calibration data if loaded
    )
    print("Quantization complete.")
except Exception as e:
    print(f"Error during quantization: {e}")
    exit(1)


# --- Save Quantized Model ---
# The save_quantized method saves the quantized weights,
# necessary config files (config.json, quantization_config.json), and tokenizer files.
print(f"Saving quantized model to: {quant_path}")

try:
    model.save_quantized(quant_path)
    # Tokenizer is usually saved automatically, but double-check if needed:
    # tokenizer.save_pretrained(quant_path)
    print("Quantized model saved successfully.")
except Exception as e:
    print(f"Error saving quantized model: {e}")
    exit(1)

print("\n--- Process Finished ---")
print(f"Quantized model (AWQ, 4-bit) saved at: {quant_path}")
print("You can now use this path with vLLM using '--quantization awq'")
print("Example vLLM command snippet:")
print(f"python -m vllm.entrypoints.openai.api_server --model {quant_path} --quantization awq --tensor-parallel-size 8 ...")


python -m vllm.entrypoints.openai.api_server \
    --model /path/to/save/quantized/llama-3.1-70b-instruct-awq \
    --quantization awq \
    --tensor-parallel-size 8 `# Or your desired number of GPUs` \
    --host 0.0.0.0 \
    --port 8000 \
    --gpu-memory-utilization 0.9 \
    --max-model-len 8192 `# Adjust as needed` \
    --trust-remote-code
