from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import torch.nn.functional as F

# Load the model and tokenizer
model_name = "meta-llama/Meta-Llama-3-8B"  # Adjust based on your model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16  # Use float16 if bfloat16 isn't supported
)
model.eval()

# Tokenize input prompt
prompt = "The capital of France is"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

# Generate text with output scores
output = model.generate(
    input_ids,
    max_new_tokens=5,  # Number of new tokens to generate
    return_dict_in_generate=True,
    output_scores=True,
)

# Extract generated tokens (excluding the input)
generated_sequence = output.sequences[0]
input_length = input_ids.shape[1]
generated_tokens = generated_sequence[input_length:]

# Calculate log probabilities for each generated token
log_probs = []
for i, token in enumerate(generated_tokens):
    # Get logits for the i-th generated token
    logits = output.scores[i][0]  # Shape: [vocab_size]
    # Compute log probabilities using log_softmax
    log_prob = F.log_softmax(logits, dim=-1)[token].item()
    log_probs.append(log_prob)

# Decode and print results
print("Generated Text:", tokenizer.decode(generated_sequence, skip_special_tokens=True))
print("Generated Tokens:", tokenizer.convert_ids_to_tokens(generated_tokens))
print("Log Probabilities:", log_probs)
