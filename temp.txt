import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    precision_recall_curve, roc_curve, auc,
    f1_score, precision_score, recall_score, accuracy_score,
    confusion_matrix, classification_report, matthews_corrcoef
)
import matplotlib.pyplot as plt
import seaborn as sns


# Use precision_recall_curve to get thresholds and corresponding precision/recall
precision, recall, thresholds_pr = precision_recall_curve(y_true, y_pred_proba)

# Calculate F1 score for each threshold (adding a small epsilon to avoid division by zero)
f1_scores = (2 * precision * recall) / (precision + recall + 1e-9)

# Exclude the last precision/recall pair as it corresponds to threshold 1.0 (recall=0)
f1_scores = f1_scores[:-1]
thresholds_pr = thresholds_pr[:len(f1_scores)] # Adjust thresholds array length

# Find the threshold that gives the maximum F1 score
optimal_idx_f1 = np.argmax(f1_scores)
optimal_threshold_f1 = thresholds_pr[optimal_idx_f1]
optimal_f1 = f1_scores[optimal_idx_f1]

print(f"Optimal Threshold (Max F1): {optimal_threshold_f1:.4f}")
print(f"Maximum F1 Score: {optimal_f1:.4f}")

# Alternative: Maximize G-Mean using ROC curve data
fpr, tpr, thresholds_roc = roc_curve(y_true, y_pred_proba)
tnr = 1 - fpr # Specificity
gmean = np.sqrt(tpr * tnr)

# Find threshold maximizing G-Mean (adjust index as thresholds_roc has different length)
optimal_idx_gmean = np.argmax(gmean)
optimal_threshold_gmean = thresholds_roc[optimal_idx_gmean]
optimal_gmean = gmean[optimal_idx_gmean]

print(f"Optimal Threshold (Max G-Mean): {optimal_threshold_gmean:.4f}")
print(f"Maximum G-Mean: {optimal_gmean:.4f}")

# --- Choose your optimal threshold based on your goal ---
# Let's proceed using the F1-optimized threshold for demonstration
optimal_threshold = optimal_threshold_f1



# Apply the chosen optimal threshold
y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)

# Calculate metrics at this threshold
print("\n--- Metrics at Optimal Threshold ({:.4f}) ---".format(optimal_threshold))

cm = confusion_matrix(y_true, y_pred_optimal)
print("Confusion Matrix:")
print(cm)

# Visualize Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Neg', 'Predicted Pos'], yticklabels=['Actual Neg', 'Actual Pos'])
plt.title(f'Confusion Matrix (Threshold = {optimal_threshold:.2f})')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()


print("\nClassification Report:")
print(classification_report(y_true, y_pred_optimal, target_names=['Negative (0)', 'Positive (1)']))

# Calculate individual metrics explicitly if needed
accuracy = accuracy_score(y_true, y_pred_optimal)
precision_opt = precision_score(y_true, y_pred_optimal)
recall_opt = recall_score(y_true, y_pred_optimal)
f1_opt = f1_score(y_true, y_pred_optimal)
mcc_opt = matthews_corrcoef(y_true, y_pred_optimal)
tn, fp, fn, tp = cm.ravel()
specificity_opt = tn / (tn + fp)
gmean_opt = np.sqrt(recall_opt * specificity_opt)


print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision_opt:.4f}")
print(f"Recall (Sensitivity): {recall_opt:.4f}")
print(f"F1 Score: {f1_opt:.4f}")
print(f"Specificity: {specificity_opt:.4f}")
print(f"G-Mean: {gmean_opt:.4f}")
print(f"MCC: {mcc_opt:.4f}")



# --- ROC Curve ---
fpr, tpr, thresholds_roc = roc_curve(y_true, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance')

# Find the closest threshold on the ROC curve to our chosen optimal_threshold
idx_roc_optimal = np.argmin(np.abs(thresholds_roc - optimal_threshold))
plt.plot(fpr[idx_roc_optimal], tpr[idx_roc_optimal], 'ro', markersize=8, label=f'Optimal Threshold ≈ {optimal_threshold:.2f}')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()
print(f"AUC-ROC: {roc_auc:.4f}")


# --- Precision-Recall Curve ---
precision, recall, thresholds_pr = precision_recall_curve(y_true, y_pred_proba)
pr_auc = auc(recall, precision) # Note order: recall is x, precision is y for auc calc

# Baseline = prevalence of positive class
baseline = pos_count / (pos_count + neg_count)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.2f})')
plt.plot([0, 1], [baseline, baseline], color='red', lw=2, linestyle='--', label=f'Baseline ({baseline:.2f})')

# Find the closest threshold on the PR curve to our chosen optimal_threshold
# Need to handle threshold array length carefully
valid_threshold_indices = np.where(thresholds_pr >= optimal_threshold)[0]
if len(valid_threshold_indices) > 0:
    idx_pr_optimal = valid_threshold_indices[0]
    # Ensure index is within bounds for precision/recall (which are 1 longer than thresholds_pr)
    if idx_pr_optimal < len(recall) and idx_pr_optimal < len(precision):
         plt.plot(recall[idx_pr_optimal], precision[idx_pr_optimal], 'go', markersize=8, label=f'Optimal Threshold ≈ {optimal_threshold:.2f}')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.legend(loc="lower left")
plt.grid(alpha=0.3)
plt.show()
print(f"AUC-PR: {pr_auc:.4f}")


# --- Probability Distribution Plot ---
plt.figure(figsize=(10, 6))
sns.histplot(y_pred_proba[y_true == 0], color="skyblue", label="Negative Class (0)", kde=True, stat='density', common_norm=False)
sns.histplot(y_pred_proba[y_true == 1], color="red", label="Positive Class (1)", kde=True, stat='density', common_norm=False)
plt.axvline(optimal_threshold, color='black', linestyle='--', label=f'Optimal Threshold = {optimal_threshold:.2f}')
plt.title('Distribution of Predicted Probabilities')
plt.xlabel('Predicted Probability of Positive Class')
plt.ylabel('Density')
plt.legend()
plt.show()

