from typing import Any, List, Optional, Dict
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_community.chat_models.huggingface import ChatHuggingFace

class LogprobChatHuggingFace(ChatHuggingFace):
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> ChatResult:
        # Force return_full_text=False to isolate new tokens
        response = self.llm.client(
            [self._to_chat_prompt(messages)],
            stop=stop,
            return_full_text=False,  # Critical for token alignment
            return_dict_in_generate=True,
            output_scores=True,
            **kwargs,
        )

        # Extract generated text
        generated_text = response["generated_text"][0]

        # Store Hugging Face's raw output in ChatGeneration
        return ChatResult(
            generations=[
                ChatGeneration(
                    message=AIMessage(content=generated_text),
                    generation_info={
                        "hf_output": response  # Raw model output here
                    }
                )
            ]
        )

    @property
    def _llm_type(self) -> str:
        return "huggingface_chat_with_logprobs"
